#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬
ä½ç½®: psychological_counseling_system/evaluation/scripts/run_quick_test.py

å¿«é€ŸéªŒè¯ç³»ç»ŸåŠŸèƒ½ï¼Œ10ä¸ªæ ·æœ¬ï¼Œ5åˆ†é’Ÿå®Œæˆ

ä½¿ç”¨æ–¹æ³•:
    python evaluation/scripts/run_quick_test.py
    
    # æŒ‡å®šæ ·æœ¬æ•°
    python evaluation/scripts/run_quick_test.py --samples 20
    
    # æŒ‡å®šé…ç½®æ–‡ä»¶
    python evaluation/scripts/run_quick_test.py --config evaluation/configs/quick_test_config.yaml
"""

import os
import sys
import argparse
import logging
import yaml
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from evaluation import EvaluationFramework
from dialogue import create_dialogue_manager_from_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_file: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def print_banner():
    """æ‰“å°æ¨ªå¹…"""
    print("\n" + "="*70)
    print(" "*20 + "å¿«é€Ÿæµ‹è¯• - å¿ƒç†å’¨è¯¢ç³»ç»Ÿè¯„ä¼°")
    print("="*70)


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "-"*70)
    print(f" {title}")
    print("-"*70)


def run_quick_test(
    config_file: str = None,
    num_samples: int = 10,
    save_results: bool = True
):
    """
    è¿è¡Œå¿«é€Ÿæµ‹è¯•
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        num_samples: æµ‹è¯•æ ·æœ¬æ•°
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
    """
    print_banner()
    
    # 1. åŠ è½½é…ç½®
    print_section("1. åŠ è½½é…ç½®")
    
    if config_file is None:
        config_file = project_root / "configs" / "config.yaml"
    
    logger.info(f"é…ç½®æ–‡ä»¶: {config_file}")
    
    try:
        system_config = load_config(config_file)
        logger.info("âœ“ ç³»ç»Ÿé…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None
    
    # åŠ è½½è¯„ä¼°é…ç½®
    eval_config_file = project_root / "evaluation" / "configs" / "quick_test_config.yaml"
    if eval_config_file.exists():
        eval_config = load_config(eval_config_file)
        logger.info("âœ“ è¯„ä¼°é…ç½®åŠ è½½æˆåŠŸ")
    else:
        eval_config = {}
        logger.warning("æœªæ‰¾åˆ°è¯„ä¼°é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
    
    # 2. åˆå§‹åŒ–ç³»ç»Ÿ
    print_section("2. åˆå§‹åŒ–ç³»ç»Ÿ")
    
    try:
        dialogue_manager = create_dialogue_manager_from_config(system_config)
        logger.info("âœ“ å¯¹è¯ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œä¾èµ–é¡¹")
        return None
    
    # 3. åˆ›å»ºè¯„ä¼°æ¡†æ¶
    print_section("3. åˆ›å»ºè¯„ä¼°æ¡†æ¶")
    
    data_dir = project_root / "data"
    output_dir = project_root / "evaluation" / "results" / "quick_test"
    
    evaluator = EvaluationFramework(
        dialogue_manager=dialogue_manager,
        data_dir=str(data_dir),
        output_dir=str(output_dir)
    )
    logger.info("âœ“ è¯„ä¼°æ¡†æ¶åˆ›å»ºæˆåŠŸ")
    
    # 4. å‡†å¤‡æµ‹è¯•æ•°æ®
    print_section("4. å‡†å¤‡æµ‹è¯•æ•°æ®")
    
    logger.info(f"åŠ è½½ MentalChat16K æ•°æ®é›†ï¼ˆ{num_samples}ä¸ªæ ·æœ¬ï¼‰...")
    
    try:
        eval_set = evaluator.load_mentalchat_dataset(num_test_samples=num_samples)
        logger.info(f"âœ“ æˆåŠŸåŠ è½½ {len(eval_set['questions'])} ä¸ªæµ‹è¯•é—®é¢˜")
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        logger.error("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        logger.error("1. è¿è¡Œæ•°æ®é›†ä¸‹è½½è„šæœ¬:")
        logger.error("   python evaluation/datasets/download_datasets.py --dataset mentalchat")
        logger.error("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return None
    
    # 5. è¿è¡Œè¯„ä¼°
    print_section("5. è¿è¡Œè¯„ä¼°")
    
    logger.info("å¼€å§‹è¯„ä¼°...")
    logger.info("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    start_time = datetime.now()
    
    try:
        results = evaluator.quick_test(num_samples=num_samples)
        
        end_time = datetime.now()
        elapsed_time = (end_time - start_time).total_seconds()
        
        logger.info(f"âœ“ è¯„ä¼°å®Œæˆï¼ç”¨æ—¶: {elapsed_time:.1f} ç§’")
        
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # 6. æ˜¾ç¤ºç»“æœ
    print_section("6. æµ‹è¯•ç»“æœ")
    
    print("\nğŸ“Š æŠ€æœ¯æŒ‡æ ‡:")
    if 'technical' in results:
        tech = results['technical']
        if 'bert_score' in tech:
            bert = tech['bert_score']
            print(f"  BERT Score F1:  {bert.get('f1', 0):.3f}")
            print(f"  BERT Precision: {bert.get('precision', 0):.3f}")
            print(f"  BERT Recall:    {bert.get('recall', 0):.3f}")
        
        if 'rouge' in tech:
            rouge = tech['rouge']
            print(f"  ROUGE-1:        {rouge.get('rouge1', 0):.3f}")
            print(f"  ROUGE-L:        {rouge.get('rougeL', 0):.3f}")
        
        if 'response_stats' in tech:
            stats = tech['response_stats']
            print(f"  å¹³å‡å“åº”æ—¶é—´:    {stats.get('avg_time', 0):.2f} ç§’")
            print(f"  å¹³å‡å“åº”é•¿åº¦:    {stats.get('avg_length', 0):.0f} å­—ç¬¦")
    
    print("\nğŸ“‹ ä¸“ä¸šè´¨é‡:")
    if 'clinical' in results:
        clinical = results['clinical']
        if isinstance(clinical, dict):
            for dimension, score in clinical.items():
                if isinstance(score, (int, float)):
                    print(f"  {dimension.capitalize():15s} {score:.2f}/5")
    
    print("\nğŸ§  è®°å¿†ç³»ç»Ÿ:")
    if 'memory' in results:
        memory = results['memory']
        if isinstance(memory, dict):
            if 'short_term_recall' in memory:
                print(f"  çŸ­æœŸè®°å¿†å¬å›:    {memory['short_term_recall']:.2%}")
            if 'accuracy' in memory:
                print(f"  æ•´ä½“å‡†ç¡®ç‡:      {memory['accuracy']:.2%}")
    
    print("\nğŸ” RAGæ•ˆæœ:")
    if 'rag' in results:
        rag = results['rag']
        if isinstance(rag, dict):
            if 'recall' in rag:
                print(f"  æ£€ç´¢å¬å›ç‡:      {rag['recall']:.2%}")
            if 'precision' in rag:
                print(f"  æ£€ç´¢ç²¾ç¡®ç‡:      {rag['precision']:.2%}")
    
    # 7. ä¿å­˜ç»“æœ
    if save_results:
        print_section("7. ä¿å­˜ç»“æœ")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"quick_test_{timestamp}.json"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ ç»“æœå·²ä¿å­˜: {output_file}")
    
    # æ€»ç»“
    print("\n" + "="*70)
    print(" "*25 + "æµ‹è¯•å®Œæˆï¼")
    print("="*70)
    print(f"\nâœ“ æµ‹è¯•æ ·æœ¬æ•°: {num_samples}")
    print(f"âœ“ ç”¨æ—¶: {elapsed_time:.1f} ç§’")
    if save_results:
        print(f"âœ“ ç»“æœæ–‡ä»¶: {output_file}")
    
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹è¯¦ç»†ç»“æœ: cat", output_file)
    print("  2. è¿è¡Œå®Œæ•´è¯„ä¼°: python evaluation/scripts/run_full_evaluation.py")
    print("  3. è¿è¡Œå¯¹æ¯”å®éªŒ: python evaluation/scripts/run_comparison.py")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯ç³»ç»ŸåŠŸèƒ½"
    )
    parser.add_argument(
        "--config",
        default=None,
        help="ç³»ç»Ÿé…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--samples",
        type=int,
        default=10,
        help="æµ‹è¯•æ ·æœ¬æ•° (é»˜è®¤: 10)"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ä¸ä¿å­˜ç»“æœ"
    )
    
    args = parser.parse_args()
    
    try:
        results = run_quick_test(
            config_file=args.config,
            num_samples=args.samples,
            save_results=not args.no_save
        )
        
        if results is None:
            sys.exit(1)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
