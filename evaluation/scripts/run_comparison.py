#!/usr/bin/env python3
"""
å¯¹æ¯”å®éªŒè„šæœ¬ - ä¸‰ç³»ç»Ÿå¯¹æ¯”è¯„ä¼°

å¯¹æ¯”ä¸‰ç§é…ç½®ï¼š
1. è£¸LLMï¼ˆåŸºçº¿ï¼‰
2. LLM + RAG
3. å®Œæ•´ç³»ç»Ÿï¼ˆLLM + RAG + è®°å¿†ï¼‰

ç”¨é€”ï¼š
- è®ºæ–‡çš„æ¶ˆèå®éªŒ
- è¯æ˜RAGå’Œè®°å¿†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§
- ç”Ÿæˆå¯¹æ¯”æ•°æ®å’Œå›¾è¡¨

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluation/scripts/run_comparison.py
    python evaluation/scripts/run_comparison.py --samples 50
"""

import sys
import os
import json
import yaml
import argparse
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from copy import deepcopy

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evaluation.framework import EvaluationFramework
from evaluation.datasets.mentalchat_loader import MentalChatLoader
from dialogue.manager import DialogueManager


class ComparisonExperiment:
    """å¯¹æ¯”å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, system_config_path: str, eval_config_path: str):
        """
        åˆå§‹åŒ–å¯¹æ¯”å®éªŒ
        
        Args:
            system_config_path: ç³»ç»Ÿé…ç½®æ–‡ä»¶è·¯å¾„
            eval_config_path: è¯„ä¼°é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.system_config_path = system_config_path
        self.eval_config_path = eval_config_path
        
        # åŠ è½½é…ç½®
        with open(system_config_path, 'r', encoding='utf-8') as f:
            self.base_config = yaml.safe_load(f)
        
        with open(eval_config_path, 'r', encoding='utf-8') as f:
            self.eval_config = yaml.safe_load(f)
        
        # ä¸‰ç§é…ç½®
        self.configs = {
            'baseline': self._create_baseline_config(),
            'rag_only': self._create_rag_only_config(),
            'full_system': self._create_full_system_config()
        }
        
        self.results = {}
    
    def _create_baseline_config(self) -> Dict:
        """åˆ›å»ºè£¸LLMé…ç½®ï¼ˆç¦ç”¨RAGå’Œè®°å¿†ï¼‰"""
        config = deepcopy(self.base_config)
        
        # ç¦ç”¨RAG
        if 'rag' in config:
            config['rag']['enabled'] = False
        
        # ç¦ç”¨è®°å¿†
        if 'memory' in config:
            config['memory']['enabled'] = False
        
        return config
    
    def _create_rag_only_config(self) -> Dict:
        """åˆ›å»ºLLM+RAGé…ç½®ï¼ˆç¦ç”¨è®°å¿†ï¼‰"""
        config = deepcopy(self.base_config)
        
        # å¯ç”¨RAG
        if 'rag' in config:
            config['rag']['enabled'] = True
        
        # ç¦ç”¨è®°å¿†
        if 'memory' in config:
            config['memory']['enabled'] = False
        
        return config
    
    def _create_full_system_config(self) -> Dict:
        """åˆ›å»ºå®Œæ•´ç³»ç»Ÿé…ç½®ï¼ˆå¯ç”¨æ‰€æœ‰åŠŸèƒ½ï¼‰"""
        config = deepcopy(self.base_config)
        
        # å¯ç”¨RAG
        if 'rag' in config:
            config['rag']['enabled'] = True
        
        # å¯ç”¨è®°å¿†
        if 'memory' in config:
            config['memory']['enabled'] = True
        
        return config
    
    def run_experiment(self, num_samples: int = None) -> Dict[str, Dict]:
        """
        è¿è¡Œå¯¹æ¯”å®éªŒ
        
        Args:
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
            
        Returns:
            ä¸‰ç§é…ç½®çš„è¯„ä¼°ç»“æœ
        """
        print("\n" + "="*70)
        print(" "*20 + "å¯¹æ¯”å®éªŒ - ä¸‰ç³»ç»Ÿè¯„ä¼°")
        print("="*70)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        print("\n" + "-"*70)
        print(" 1. åŠ è½½æµ‹è¯•æ•°æ®")
        print("-"*70)
        
        if num_samples is None:
            num_samples = self.eval_config.get('num_test_samples', 50)
        
        loader = MentalChatLoader()
        test_questions = loader.get_test_split(num_samples=num_samples)
        print(f"âœ“ åŠ è½½ {len(test_questions)} ä¸ªæµ‹è¯•é—®é¢˜")
        
        # ä¾æ¬¡è¿è¡Œä¸‰ç§é…ç½®
        config_names = {
            'baseline': 'è£¸LLMï¼ˆåŸºçº¿ï¼‰',
            'rag_only': 'LLM + RAG',
            'full_system': 'å®Œæ•´ç³»ç»Ÿï¼ˆLLM + RAG + è®°å¿†ï¼‰'
        }
        
        total_start_time = time.time()
        
        for config_name, config_desc in config_names.items():
            print("\n" + "="*70)
            print(f" æ­£åœ¨è¯„ä¼°: {config_desc}")
            print("="*70)
            
            start_time = time.time()

            
                
            try:
                # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
                print(f"\nåˆå§‹åŒ– {config_desc}...")
                
                # åˆ›å»ºLLM
                from llm.factory import create_llm_from_config
                llm = create_llm_from_config(self.configs[config_name])

                # åˆ›å»ºRAGç®¡ç†å™¨
                from knowledge.rag_manager import RAGManager
                from knowledge.chroma_kb import ChromaKnowledgeBase
                
                rag_config = self.configs[config_name].get('rag', {})
                
                # ä¸ºä¸¤ä¸ªçŸ¥è¯†åº“å‡†å¤‡é…ç½®
                psych_kb_config = {
                    'collection_name': 'psychological_knowledge',
                    'persist_directory': rag_config.get('persist_directory', './data/chroma_db'),
                    'embedding': rag_config.get('embedding', {})
                }
                
                user_kb_config = {
                    'collection_name': 'user_knowledge',
                    'persist_directory': rag_config.get('persist_directory', './data/chroma_db'),
                    'embedding': rag_config.get('embedding', {})
                }
                
                # åˆ›å»ºä¸¤ä¸ªçŸ¥è¯†åº“
                psychological_kb = ChromaKnowledgeBase(psych_kb_config)
                user_kb = ChromaKnowledgeBase(user_kb_config)
                
                # åˆ›å»ºRAGç®¡ç†å™¨
                rag_manager = RAGManager(
                    psychological_kb=psychological_kb,
                    user_kb=user_kb,
                    config=rag_config
                )
                
                # åˆ›å»ºè®°å¿†ç®¡ç†å™¨
                from memory.manager import MemoryManager
                from memory.storage import JSONMemoryStorage

                memory_config = self.configs[config_name].get('memory', {})
                storage_path = memory_config.get('storage', {}).get('path', './data/memory_db')
                storage = JSONMemoryStorage(storage_path)
                memory_manager = MemoryManager(
                    storage=storage,
                    summarizer=llm,
                    config=memory_config
                )

                # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
                dialogue_manager = DialogueManager(
                    llm=llm,
                    rag_manager=rag_manager,
                    memory_manager=memory_manager,
                    config=self.configs[config_name].get('dialogue', {})
                )
                
                # åˆ›å»ºè¯„ä¼°æ¡†æ¶
                eval_framework = EvaluationFramework(
                    dialogue_manager=dialogue_manager,
                    llm_evaluator=llm,
                    data_dir=self.eval_config.get('data_dir', './data'),
                    output_dir=self.eval_config.get('output_dir', './evaluation_results')
                )
                
                # è¿è¡Œè¯„ä¼°
                print(f"å¼€å§‹è¯„ä¼°...")
                results = eval_framework.run_full_evaluation(
                    dataset="mentalchat",
                    num_test_samples=len(test_questions),
                    generate_memory_tests=False
                )
                
                elapsed_time = time.time() - start_time
                
                # ä¿å­˜ç»“æœ
                self.results[config_name] = {
                    'config_desc': config_desc,
                    'results': results,
                    'elapsed_time': elapsed_time
                }
                
                print(f"\nâœ“ {config_desc} è¯„ä¼°å®Œæˆ")
                print(f"  ç”¨æ—¶: {elapsed_time:.1f} ç§’")
                
            except Exception as e:
                print(f"\nâœ— {config_desc} è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        total_elapsed = time.time() - total_start_time
        
        print("\n" + "="*70)
        print(" "*25 + "æ‰€æœ‰è¯„ä¼°å®Œæˆ")
        print("="*70)
        print(f"æ€»ç”¨æ—¶: {total_elapsed:.1f} ç§’ ({total_elapsed/60:.1f} åˆ†é’Ÿ)")
        
        return self.results
    
    def print_comparison(self):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*70)
        print(" "*20 + "å¯¹æ¯”ç»“æœæ‘˜è¦")
        print("="*70)
        
        # å‡†å¤‡å¯¹æ¯”æ•°æ®
        comparison_data = {}
        
        for config_name, data in self.results.items():
            results = data['results']
            comparison_data[config_name] = {
                'name': data['config_desc'],
                'tech': results.get('technical_metrics', {}),
                'clinical': results.get('clinical_metrics', {}),
                'memory': results.get('memory_metrics', {}),
                'rag': results.get('rag_metrics', {})
            }
        
        # æŠ€æœ¯æŒ‡æ ‡å¯¹æ¯”
        print("\nğŸ“Š æŠ€æœ¯æŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<20} {'è£¸LLM':<15} {'LLM+RAG':<15} {'å®Œæ•´ç³»ç»Ÿ':<15}")
        print("-"*70)
        
        tech_metrics = ['bert_f1', 'rouge_l', 'response_time']
        for metric in tech_metrics:
            values = []
            for config in ['baseline', 'rag_only', 'full_system']:
                if config in comparison_data:
                    tech = comparison_data[config]['tech']
                    if metric == 'bert_f1':
                        val = tech.get('bert_score', {}).get('f1', 0)
                    elif metric == 'rouge_l':
                        val = tech.get('rouge', {}).get('rougeL', 0)
                    elif metric == 'response_time':
                        val = tech.get('response_time', 0)
                    else:
                        val = 0
                    values.append(val)
                else:
                    values.append(0)
            
            metric_name = {
                'bert_f1': 'BERT Score F1',
                'rouge_l': 'ROUGE-L',
                'response_time': 'å“åº”æ—¶é—´(ç§’)'
            }[metric]
            
            print(f"{metric_name:<20} {values[0]:<15.3f} {values[1]:<15.3f} {values[2]:<15.3f}")
        
        # ä¸´åºŠæŒ‡æ ‡å¯¹æ¯”
        print("\nğŸ“‹ ä¸´åºŠæŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<20} {'è£¸LLM':<15} {'LLM+RAG':<15} {'å®Œæ•´ç³»ç»Ÿ':<15}")
        print("-"*70)
        
        clinical_metrics = ['empathy', 'support', 'relevance']
        for metric in clinical_metrics:
            values = []
            for config in ['baseline', 'rag_only', 'full_system']:
                if config in comparison_data:
                    val = comparison_data[config]['clinical'].get(metric, 0)
                    values.append(val)
                else:
                    values.append(0)
            
            metric_name = {
                'empathy': 'å…±æƒ…',
                'support': 'æ”¯æŒ',
                'relevance': 'ç›¸å…³æ€§'
            }[metric]
            
            print(f"{metric_name:<20} {values[0]:<15.2f} {values[1]:<15.2f} {values[2]:<15.2f}")
        
        # è®°å¿†ç³»ç»Ÿå¯¹æ¯”ï¼ˆåªæœ‰å®Œæ•´ç³»ç»Ÿæœ‰ï¼‰
        print("\nğŸ§  è®°å¿†ç³»ç»Ÿ:")
        if 'full_system' in comparison_data:
            memory = comparison_data['full_system']['memory']
            print(f"  çŸ­æœŸè®°å¿†å¬å›: {memory.get('short_term_recall', 0)*100:.2f}%")
            print(f"  æ•´ä½“å‡†ç¡®ç‡:   {memory.get('overall_accuracy', 0)*100:.2f}%")
        else:
            print("  ï¼ˆæœªå¯ç”¨ï¼‰")
        
        # RAGæ•ˆæœå¯¹æ¯”ï¼ˆrag_onlyå’Œfull_systemæœ‰ï¼‰
        print("\nğŸ” RAGæ•ˆæœ:")
        print(f"{'é…ç½®':<20} {'å¬å›ç‡':<15} {'ç²¾ç¡®ç‡':<15}")
        print("-"*70)
        
        for config in ['rag_only', 'full_system']:
            if config in comparison_data:
                rag = comparison_data[config]['rag']
                name = comparison_data[config]['name']
                recall = rag.get('recall', 0) * 100
                precision = rag.get('precision', 0) * 100
                print(f"{name:<20} {recall:<15.2f} {precision:<15.2f}")
    
    def calculate_improvements(self) -> Dict:
        """è®¡ç®—æ”¹è¿›å¹…åº¦"""
        improvements = {}
        
        if 'baseline' not in self.results or 'full_system' not in self.results:
            return improvements
        
        baseline = self.results['baseline']['results']
        full = self.results['full_system']['results']
        
        # æŠ€æœ¯æŒ‡æ ‡æ”¹è¿›
        baseline_tech = baseline.get('technical_metrics', {})
        full_tech = full.get('technical_metrics', {})
        
        # BERT Score F1
        baseline_bert = baseline_tech.get('bert_score', {}).get('f1', 0)
        full_bert = full_tech.get('bert_score', {}).get('f1', 0)
        if baseline_bert > 0:
            improvements['bert_f1'] = ((full_bert - baseline_bert) / baseline_bert) * 100
        
        # ROUGE-L
        baseline_rouge = baseline_tech.get('rouge', {}).get('rougeL', 0)
        full_rouge = full_tech.get('rouge', {}).get('rougeL', 0)
        if baseline_rouge > 0:
            improvements['rouge_l'] = ((full_rouge - baseline_rouge) / baseline_rouge) * 100
        
        # ä¸´åºŠæŒ‡æ ‡æ”¹è¿›
        baseline_clinical = baseline.get('clinical_metrics', {})
        full_clinical = full.get('clinical_metrics', {})
        
        for metric in ['empathy', 'support', 'relevance']:
            baseline_val = baseline_clinical.get(metric, 0)
            full_val = full_clinical.get(metric, 0)
            if baseline_val > 0:
                improvements[metric] = ((full_val - baseline_val) / baseline_val) * 100
        
        return improvements
    
    def print_improvements(self):
        """æ‰“å°æ”¹è¿›å¹…åº¦"""
        improvements = self.calculate_improvements()
        
        if not improvements:
            print("\nâš ï¸  æ— æ³•è®¡ç®—æ”¹è¿›å¹…åº¦ï¼ˆç¼ºå°‘åŸºçº¿æˆ–å®Œæ•´ç³»ç»Ÿç»“æœï¼‰")
            return
        
        print("\n" + "="*70)
        print(" "*20 + "å®Œæ•´ç³»ç»Ÿç›¸æ¯”è£¸LLMçš„æ”¹è¿›")
        print("="*70)
        
        print("\nğŸ“ˆ æ”¹è¿›å¹…åº¦:")
        for metric, improvement in improvements.items():
            metric_name = {
                'bert_f1': 'BERT Score F1',
                'rouge_l': 'ROUGE-L',
                'empathy': 'å…±æƒ…',
                'support': 'æ”¯æŒ',
                'relevance': 'ç›¸å…³æ€§'
            }.get(metric, metric)
            
            sign = '+' if improvement > 0 else ''
            print(f"  {metric_name:<20} {sign}{improvement:>6.2f}%")
    
    def save_results(self, output_dir: str = None) -> str:
        """ä¿å­˜å¯¹æ¯”å®éªŒç»“æœ"""
        if output_dir is None:
            output_dir = "evaluation/results/comparison"
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = output_path / f"comparison_{timestamp}.json"
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'metadata': {
                'timestamp': timestamp,
                'num_samples': self.eval_config.get('num_test_samples', 50),
                'system_config': self.system_config_path,
                'eval_config': self.eval_config_path
            },
            'results': self.results,
            'improvements': self.calculate_improvements()
        }
        
        # ä¿å­˜JSON
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {result_file}")
        
        return str(result_file)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œä¸‰ç³»ç»Ÿå¯¹æ¯”å®éªŒ')
    parser.add_argument('--samples', type=int, default=50,
                       help='æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤50ï¼‰')
    parser.add_argument('--system-config', type=str,
                       default='configs/config.yaml',
                       help='ç³»ç»Ÿé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--eval-config', type=str,
                       default='evaluation/configs/default_config.yaml',
                       help='è¯„ä¼°é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜ç»“æœ')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¯¹æ¯”å®éªŒ
        experiment = ComparisonExperiment(args.system_config, args.eval_config)
        
        # è¿è¡Œå®éªŒ
        results = experiment.run_experiment(args.samples)
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        experiment.print_comparison()
        
        # æ‰“å°æ”¹è¿›å¹…åº¦
        experiment.print_improvements()
        
        # ä¿å­˜ç»“æœ
        if not args.no_save:
            result_file = experiment.save_results(args.output_dir)
        
        # å®Œæˆ
        print("\n" + "="*70)
        print(" "*25 + "å¯¹æ¯”å®éªŒå®Œæˆï¼")
        print("="*70)
        
        if not args.no_save:
            print(f"\nâœ“ ç»“æœæ–‡ä»¶: {result_file}")
        print(f"âœ“ æµ‹è¯•æ ·æœ¬æ•°: {args.samples}")
        print(f"âœ“ é…ç½®æ•°é‡: 3")
        
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("  1. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨: python evaluation/scripts/visualize_comparison.py")
        print("  2. ç”ŸæˆLaTeXæŠ¥å‘Š: python evaluation/reporting/generate_latex_report.py")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâœ— å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
