#!/usr/bin/env python3
"""
å®Œæ•´è¯„ä¼°è„šæœ¬ - è¿è¡Œ200æ ·æœ¬çš„å®Œæ•´ç³»ç»Ÿè¯„ä¼°

ç”¨é€”ï¼š
1. è®ºæ–‡å‘è¡¨çš„æ ‡å‡†è¯„ä¼°
2. æ‰€æœ‰21ä¸ªæŒ‡æ ‡çš„å…¨é¢æµ‹è¯•
3. è‡ªåŠ¨ä¿å­˜è¯¦ç»†ç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluation/scripts/run_full_evaluation.py
    python evaluation/scripts/run_full_evaluation.py --samples 200 --config evaluation/configs/full_eval_config.yaml
"""

import sys
import os
import json
import yaml
import argparse
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evaluation.framework import EvaluationFramework
from evaluation.datasets.mentalchat_loader import MentalChatLoader
from dialogue.manager import DialogueManager


class FullEvaluator:
    """å®Œæ•´è¯„ä¼°ç®¡ç†å™¨"""
    
    def __init__(self, system_config_path: str, eval_config_path: str):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            system_config_path: ç³»ç»Ÿé…ç½®æ–‡ä»¶è·¯å¾„
            eval_config_path: è¯„ä¼°é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.system_config_path = system_config_path
        self.eval_config_path = eval_config_path
        
        # åŠ è½½é…ç½®
        self.system_config = self._load_config(system_config_path)
        self.eval_config = self._load_config(eval_config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.dialogue_manager = None
        self.eval_framework = None
        self.results = {}
        
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    # def initialize(self):
    #     """åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ"""
    #     print("\n" + "="*70)
    #     print(" "*20 + "å®Œæ•´è¯„ä¼° - å¿ƒç†å’¨è¯¢ç³»ç»Ÿ")
    #     print("="*70)
        
    #     # 1. åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨
    #     print("\n" + "-"*70)
    #     print(" 1. åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨")
    #     print("-"*70)
        
    #     try:
    #         self.dialogue_manager = DialogueManager(self.system_config)
    #         print("âœ“ å¯¹è¯ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    #     except Exception as e:
    #         print(f"âœ— å¯¹è¯ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    #         raise
        
    #     # 2. åˆ›å»ºè¯„ä¼°æ¡†æ¶
    #     print("\n" + "-"*70)
    #     print(" 2. åˆ›å»ºè¯„ä¼°æ¡†æ¶")
    #     print("-"*70)
        
    #     try:
    #         self.eval_framework = EvaluationFramework(
    #             dialogue_manager=self.dialogue_manager,
    #             config=self.eval_config
    #         )
    #         print("âœ“ è¯„ä¼°æ¡†æ¶åˆ›å»ºæˆåŠŸ")
    #     except Exception as e:
    #         print(f"âœ— è¯„ä¼°æ¡†æ¶åˆ›å»ºå¤±è´¥: {e}")
    #         raise
    
    def initialize(self):
        """åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ"""
        print("\n" + "="*70)
        print(" "*20 + "å®Œæ•´è¯„ä¼° - å¿ƒç†å’¨è¯¢ç³»ç»Ÿ")
        print("="*70)
        
        # 1. åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨
        print("\n" + "-"*70)
        print(" 1. åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨")
        print("-"*70)
        
        try:
            # åˆ›å»ºLLM
            from llm.factory import create_llm_from_config
            llm = create_llm_from_config(self.system_config)

            # åˆ›å»ºRAGç®¡ç†å™¨
            from knowledge.rag_manager import RAGManager
            rag_config = self.system_config.get('rag', {})
            rag_manager = RAGManager(
                llm=llm,
                config=rag_config
            )

            # åˆ›å»ºè®°å¿†ç®¡ç†å™¨
            from memory.manager import MemoryManager
            memory_config = self.system_config.get('memory', {})
            memory_manager = MemoryManager(config=memory_config)

            # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
            self.dialogue_manager = DialogueManager(
                llm=llm,
                rag_manager=rag_manager,
                memory_manager=memory_manager,
                config=self.system_config.get('dialogue', {})
            )
            
            print("âœ“ å¯¹è¯ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— å¯¹è¯ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
            

    def load_test_data(self, num_samples: int = None) -> List[Dict]:
        """
        åŠ è½½æµ‹è¯•æ•°æ®
        
        Args:
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            æµ‹è¯•é—®é¢˜åˆ—è¡¨
        """
        print("\n" + "-"*70)
        print(" 3. åŠ è½½æµ‹è¯•æ•°æ®")
        print("-"*70)
        
        if num_samples is None:
            num_samples = self.eval_config.get('num_test_samples', 200)
        
        try:
            # ä»MentalChat16KåŠ è½½æµ‹è¯•æ•°æ®
            loader = MentalChatLoader()
            test_questions = loader.get_test_questions(num_samples=num_samples)
            
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(test_questions)} ä¸ªæµ‹è¯•é—®é¢˜")
            print(f"  æ•°æ®é›†: {dataset_name}")
            print(f"  æ ·æœ¬æ•°: {len(test_questions)}")
            
            return test_questions
            
        except Exception as e:
            print(f"âœ— åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            raise
    
    def run_evaluation(self, test_questions: List[Dict]) -> Dict:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Args:
            test_questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        print("\n" + "-"*70)
        print(" 4. è¿è¡Œè¯„ä¼°")
        print("-"*70)
        
        start_time = time.time()
        
        try:
            # è¿è¡Œè¯„ä¼°
            self.results = self.eval_framework.evaluate(test_questions)
            
            elapsed_time = time.time() - start_time
            
            print(f"\nâœ“ è¯„ä¼°å®Œæˆï¼")
            print(f"  ç”¨æ—¶: {elapsed_time:.1f} ç§’ ({elapsed_time/60:.1f} åˆ†é’Ÿ)")
            print(f"  å¹³å‡æ¯æ ·æœ¬: {elapsed_time/len(test_questions):.2f} ç§’")
            
            return self.results
            
        except Exception as e:
            print(f"\nâœ— è¯„ä¼°å¤±è´¥: {e}")
            raise
    
    def print_results(self, results: Dict):
        """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦"""
        print("\n" + "-"*70)
        print(" 5. è¯„ä¼°ç»“æœ")
        print("-"*70)
        
        # æŠ€æœ¯æŒ‡æ ‡
        if 'technical_metrics' in results:
            print("\nğŸ“Š æŠ€æœ¯æŒ‡æ ‡:")
            tech = results['technical_metrics']
            
            if 'bert_score' in tech:
                bert = tech['bert_score']
                print(f"  BERT Score F1:  {bert.get('f1', 0):.3f}")
                print(f"  BERT Precision: {bert.get('precision', 0):.3f}")
                print(f"  BERT Recall:    {bert.get('recall', 0):.3f}")
            
            if 'rouge' in tech:
                rouge = tech['rouge']
                print(f"  ROUGE-1:        {rouge.get('rouge1', 0):.3f}")
                print(f"  ROUGE-2:        {rouge.get('rouge2', 0):.3f}")
                print(f"  ROUGE-L:        {rouge.get('rougeL', 0):.3f}")
            
            if 'bleu' in tech:
                print(f"  BLEU:           {tech['bleu']:.3f}")
            
            if 'response_time' in tech:
                print(f"  å¹³å‡å“åº”æ—¶é—´:    {tech['response_time']:.2f} ç§’")
            
            if 'response_length' in tech:
                print(f"  å¹³å‡å“åº”é•¿åº¦:    {tech['response_length']:.0f} å­—ç¬¦")
        
        # ä¸´åºŠæŒ‡æ ‡
        if 'clinical_metrics' in results:
            print("\nğŸ“‹ ä¸“ä¸šè´¨é‡:")
            clinical = results['clinical_metrics']
            
            metrics_names = {
                'empathy': 'å…±æƒ…',
                'support': 'æ”¯æŒ',
                'guidance': 'æŒ‡å¯¼',
                'relevance': 'ç›¸å…³æ€§',
                'communication': 'æ²Ÿé€š',
                'fluency': 'æµç•…æ€§',
                'safety': 'å®‰å…¨æ€§'
            }
            
            for key, name in metrics_names.items():
                if key in clinical:
                    score = clinical[key]
                    print(f"  {name:12s} {score:.2f}/5.0")
        
        # è®°å¿†æŒ‡æ ‡
        if 'memory_metrics' in results:
            print("\nğŸ§  è®°å¿†ç³»ç»Ÿ:")
            memory = results['memory_metrics']
            
            if 'short_term_recall' in memory:
                print(f"  çŸ­æœŸè®°å¿†å¬å›:    {memory['short_term_recall']*100:.2f}%")
            if 'working_memory_accuracy' in memory:
                print(f"  å·¥ä½œè®°å¿†å‡†ç¡®ç‡:  {memory['working_memory_accuracy']*100:.2f}%")
            if 'long_term_consistency' in memory:
                print(f"  é•¿æœŸè®°å¿†ä¸€è‡´æ€§:  {memory['long_term_consistency']*100:.2f}%")
            if 'overall_accuracy' in memory:
                print(f"  æ•´ä½“å‡†ç¡®ç‡:      {memory['overall_accuracy']*100:.2f}%")
        
        # RAGæŒ‡æ ‡
        if 'rag_metrics' in results:
            print("\nğŸ” RAGæ•ˆæœ:")
            rag = results['rag_metrics']
            
            if 'recall' in rag:
                print(f"  æ£€ç´¢å¬å›ç‡:      {rag['recall']*100:.2f}%")
            if 'precision' in rag:
                print(f"  æ£€ç´¢ç²¾ç¡®ç‡:      {rag['precision']*100:.2f}%")
            if 'f1' in rag:
                print(f"  F1 Score:       {rag['f1']:.3f}")
        
        # å®‰å…¨æ€§æŒ‡æ ‡
        if 'safety_metrics' in results:
            print("\nğŸ›¡ï¸ å®‰å…¨æ€§:")
            safety = results['safety_metrics']
            
            if 'harmful_content' in safety:
                print(f"  æœ‰å®³å†…å®¹æ£€æµ‹:    é€šè¿‡ {safety['harmful_content']*100:.1f}%")
            if 'privacy_protection' in safety:
                print(f"  éšç§ä¿æŠ¤:        é€šè¿‡ {safety['privacy_protection']*100:.1f}%")
    
    def save_results(self, results: Dict, output_dir: str = None) -> str:
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤ç›®å½•
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print("\n" + "-"*70)
        print(" 6. ä¿å­˜ç»“æœ")
        print("-"*70)
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = "evaluation/results/full_evaluation"
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = output_path / f"full_evaluation_{timestamp}.json"
        
        # æ·»åŠ å…ƒæ•°æ®
        results_with_meta = {
            'metadata': {
                'timestamp': timestamp,
                'num_samples': len(results.get('individual_results', [])),
                'system_config': self.system_config_path,
                'eval_config': self.eval_config_path
            },
            'results': results
        }
        
        # ä¿å­˜JSON
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results_with_meta, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ ç»“æœå·²ä¿å­˜: {result_file}")
        print(f"  æ–‡ä»¶å¤§å°: {result_file.stat().st_size / 1024:.1f} KB")
        
        return str(result_file)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆ200æ ·æœ¬ï¼‰')
    parser.add_argument('--samples', type=int, default=None,
                       help='æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰')
    parser.add_argument('--system-config', type=str, 
                       default='configs/config.yaml',
                       help='ç³»ç»Ÿé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--eval-config', type=str,
                       default='evaluation/configs/full_eval_config.yaml',
                       help='è¯„ä¼°é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜ç»“æœ')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = FullEvaluator(args.system_config, args.eval_config)
        
        # åˆå§‹åŒ–
        evaluator.initialize()
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_questions = evaluator.load_test_data(args.samples)
        
        # è¿è¡Œè¯„ä¼°
        results = evaluator.run_evaluation(test_questions)
        
        # æ‰“å°ç»“æœ
        evaluator.print_results(results)
        
        # ä¿å­˜ç»“æœ
        if not args.no_save:
            result_file = evaluator.save_results(results, args.output_dir)
        
        # å®Œæˆ
        print("\n" + "="*70)
        print(" "*25 + "è¯„ä¼°å®Œæˆï¼")
        print("="*70)
        
        if not args.no_save:
            print(f"\nâœ“ ç»“æœæ–‡ä»¶: {result_file}")
        print(f"âœ“ æµ‹è¯•æ ·æœ¬æ•°: {len(test_questions)}")
        print(f"âœ“ è¯„ä¼°æŒ‡æ ‡æ•°: 21")
        
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹è¯¦ç»†ç»“æœ: cat " + result_file if not args.no_save else "")
        print("  2. è¿è¡Œå¯¹æ¯”å®éªŒ: python evaluation/scripts/run_comparison.py")
        print("  3. ç”ŸæˆæŠ¥å‘Š: python evaluation/scripts/generate_report.py")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâœ— è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
