#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¯¹æ¯”å®éªŒè„šæœ¬ - æ˜“äºä½¿ç”¨çš„ä¸‰ç³»ç»Ÿå¯¹æ¯”
ä½œè€…: ä¸ºæœŸæœ«ä½œä¸šè®¾è®¡
æ—¥æœŸ: 2025-11

åŠŸèƒ½:
- å¯¹æ¯”è£¸LLMã€LLM+RAGã€å®Œæ•´ç³»ç»Ÿä¸‰ç§é…ç½®
- è‡ªåŠ¨ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
- æ”¯æŒè‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°

ä½¿ç”¨æ–¹æ³•:
    # å¿«é€Ÿè¿è¡Œï¼ˆä½¿ç”¨é»˜è®¤30ä¸ªé—®é¢˜ï¼‰
    python evaluation/scripts/simple_comparison.py
    
    # æŒ‡å®šé—®é¢˜æ•°é‡
    python evaluation/scripts/simple_comparison.py --num-questions 20
    
    # è·³è¿‡äººå·¥è¯„ä¼°
    python evaluation/scripts/simple_comparison.py --skip-manual
"""

import sys
import os
import json
import yaml
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from utils.helpers import load_config, setup_directories


class SimpleComparisonExperiment:
    """ç®€åŒ–çš„å¯¹æ¯”å®éªŒç±»"""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """åˆå§‹åŒ–å®éªŒ"""
        print("\n" + "="*70)
        print(" "*20 + "ğŸ§ª ä¸‰ç³»ç»Ÿå¯¹æ¯”å®éªŒ")
        print("="*70)
        
        # åŠ è½½é…ç½®
        print("\nğŸ“‹ æ­£åœ¨åŠ è½½é…ç½®...")
        self.config = load_config(config_path)
        
        # åŠ è½½å¯¹æ¯”å®éªŒé…ç½®
        comparison_config_path = project_root / "evaluation/configs/comparison_config.yaml"
        with open(comparison_config_path, 'r', encoding='utf-8') as f:
            self.comparison_config = yaml.safe_load(f)['comparison']
        
        # åŠ è½½æµ‹è¯•é—®é¢˜
        self.test_questions = self._load_test_questions()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'baseline': {},
            'rag_only': {},
            'full_system': {}
        }
        
        print(f"âœ“ é…ç½®åŠ è½½å®Œæˆ")
        print(f"âœ“ æµ‹è¯•é—®é¢˜æ•°: {len(self.test_questions)}")
    
    def _load_test_questions(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•é—®é¢˜"""
        questions_file = project_root / self.comparison_config['test_questions']['custom_file']
        
        with open(questions_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return data['questions']
    
    def _create_dialogue_manager(self, config_name: str):
        """åˆ›å»ºå¯¹è¯ç®¡ç†å™¨"""
        from llm.factory import create_llm_from_config
        from knowledge.rag_manager import RAGManager
        from knowledge.chroma_kb import ChromaKnowledgeBase
        from memory.manager import MemoryManager
        from memory.storage import JSONMemoryStorage
        from dialogue.manager import DialogueManager
        
        # æ ¹æ®é…ç½®åç§°è°ƒæ•´é…ç½®
        config = self.config.copy()
        comp_config = self.comparison_config['configurations'][config_name]
        
        # è°ƒæ•´RAGå¼€å…³
        if 'dialogue' not in config:
            config['dialogue'] = {}
        if 'generation' not in config['dialogue']:
            config['dialogue']['generation'] = {}
        
        config['dialogue']['generation']['enable_rag'] = comp_config['enable_rag']
        config['dialogue']['generation']['enable_memory'] = comp_config['enable_memory']
        
        # åˆ›å»ºLLM
        llm = create_llm_from_config(config)
        
        # åˆ›å»ºRAGç®¡ç†å™¨
        rag_config = config.get('rag', {})
        embedding_config = rag_config.get('embedding', {})
        persist_dir = rag_config.get('vector_store', {}).get('persist_directory', './data/vector_db')
        
        psych_kb_config = {
            'collection_name': 'psych_knowledge',
            'persist_directory': persist_dir,
            'embedding': embedding_config
        }
        
        user_kb_config = {
            'collection_name': 'user_info',
            'persist_directory': persist_dir,
            'embedding': embedding_config
        }
        
        psychological_kb = ChromaKnowledgeBase(psych_kb_config)
        user_kb = ChromaKnowledgeBase(user_kb_config)
        
        rag_manager = RAGManager(
            psychological_kb=psychological_kb,
            user_kb=user_kb,
            config=rag_config.get('retrieval', {})
        )
        
        # åˆ›å»ºè®°å¿†ç®¡ç†å™¨
        memory_config = config.get('memory', {})
        storage_path = memory_config.get('storage', {}).get('path', './data/memory_db')
        storage = JSONMemoryStorage(storage_path)
        
        memory_manager = MemoryManager(
            storage=storage,
            summarizer=llm,
            config=memory_config
        )
        
        # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
        dialogue_config = config.get('dialogue', {})
        dialogue_config['enable_rag'] = comp_config['enable_rag']
        dialogue_config['enable_memory'] = comp_config['enable_memory']
        
        dialogue_manager = DialogueManager(
            llm=llm,
            rag_manager=rag_manager,
            memory_manager=memory_manager,
            config=dialogue_config
        )
        
        return dialogue_manager
    
    def run_configuration(self, config_name: str, questions: List[Dict]) -> Dict:
        """è¿è¡Œå•ä¸ªé…ç½®çš„æµ‹è¯•"""
        comp_config = self.comparison_config['configurations'][config_name]
        config_desc = comp_config['name']
        
        print(f"\n{'='*70}")
        print(f"  ğŸ”§ æ­£åœ¨æµ‹è¯•: {config_desc}")
        print(f"{'='*70}")
        print(f"  é…ç½®è¯´æ˜: {comp_config['description']}")
        print(f"  é—®é¢˜æ•°é‡: {len(questions)}")
        
        # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
        print(f"\n  âš™ï¸  åˆå§‹åŒ–ç³»ç»Ÿ...")
        dialogue_manager = self._create_dialogue_manager(config_name)
        
        # åˆ›å»ºæµ‹è¯•ç”¨æˆ·
        user_id = f"test_user_{config_name}"
        try:
            dialogue_manager.memory_manager.create_user(
                user_id=user_id,
                age=25,
                gender="æœªçŸ¥",
                occupation="æµ‹è¯•ç”¨æˆ·"
            )
        except:
            pass  # ç”¨æˆ·å¯èƒ½å·²å­˜åœ¨
        
        session_id = dialogue_manager.start_session(user_id)
        
        # è¿è¡Œæµ‹è¯•
        responses = []
        response_times = []
        
        print(f"\n  ğŸš€ å¼€å§‹ç”Ÿæˆå›å¤...")
        
        for i, q in enumerate(questions, 1):
            question = q['question']
            
            # ç”Ÿæˆå›å¤å¹¶è®¡æ—¶
            start_time = time.time()
            try:
                response = dialogue_manager.chat(
                    user_id=user_id,
                    session_id=session_id,
                    user_message=question
                )
                elapsed = time.time() - start_time
                
                responses.append({
                    'question_id': q['id'],
                    'question': question,
                    'response': response,
                    'response_time': elapsed,
                    'category': q['category']
                })
                response_times.append(elapsed)
                
                # è¿›åº¦æ˜¾ç¤º
                if i % 5 == 0 or i == len(questions):
                    avg_time = sum(response_times) / len(response_times)
                    print(f"    è¿›åº¦: {i}/{len(questions)} | å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            
            except Exception as e:
                print(f"    âš ï¸  é—®é¢˜ {i} ç”Ÿæˆå¤±è´¥: {e}")
                responses.append({
                    'question_id': q['id'],
                    'question': question,
                    'response': "[ç”Ÿæˆå¤±è´¥]",
                    'response_time': 0,
                    'category': q['category'],
                    'error': str(e)
                })
        
        # ç»“æŸä¼šè¯
        dialogue_manager.end_session(user_id, session_id)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        valid_times = [r['response_time'] for r in responses if r['response_time'] > 0]
        valid_responses = [r for r in responses if r['response_time'] > 0]
        
        results = {
            'config_name': config_name,
            'config_desc': config_desc,
            'responses': responses,
            'statistics': {
                'total_questions': len(questions),
                'successful_responses': len(valid_responses),
                'failed_responses': len(questions) - len(valid_responses),
                'avg_response_time': sum(valid_times) / len(valid_times) if valid_times else 0,
                'min_response_time': min(valid_times) if valid_times else 0,
                'max_response_time': max(valid_times) if valid_times else 0,
                'avg_response_length': sum(len(r['response']) for r in valid_responses) / len(valid_responses) if valid_responses else 0
            }
        }
        
        print(f"\n  âœ“ {config_desc} æµ‹è¯•å®Œæˆ")
        print(f"    æˆåŠŸ: {results['statistics']['successful_responses']}/{len(questions)}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {results['statistics']['avg_response_time']:.2f}ç§’")
        
        return results
    
    def run_all_configurations(self, num_questions: int = None):
        """è¿è¡Œæ‰€æœ‰é…ç½®çš„æµ‹è¯•"""
        if num_questions is None:
            num_questions = self.comparison_config['num_test_samples']
        
        # é€‰æ‹©é—®é¢˜
        questions = self.test_questions[:num_questions]
        
        print(f"\nğŸ“ å°†ä½¿ç”¨ {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜")
        print(f"   é—®é¢˜ç±»åˆ«: {set(q['category'] for q in questions)}")
        
        # ä¾æ¬¡æµ‹è¯•ä¸‰ç§é…ç½®
        for config_name in ['baseline', 'rag_only', 'full_system']:
            self.results[config_name] = self.run_configuration(config_name, questions)
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(1)
        
        print(f"\n{'='*70}")
        print(f"  âœ… æ‰€æœ‰é…ç½®æµ‹è¯•å®Œæˆï¼")
        print(f"{'='*70}")
    
    def print_comparison(self):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print(f"\n{'='*70}")
        print(" "*20 + "ğŸ“Š å¯¹æ¯”ç»“æœæ±‡æ€»")
        print(f"{'='*70}")
        
        # è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”
        print(f"\n{'æŒ‡æ ‡':<20} {'è£¸LLM':<15} {'LLM+RAG':<15} {'å®Œæ•´ç³»ç»Ÿ':<15}")
        print("-"*70)
        
        # å“åº”æ—¶é—´
        times = [
            self.results['baseline']['statistics']['avg_response_time'],
            self.results['rag_only']['statistics']['avg_response_time'],
            self.results['full_system']['statistics']['avg_response_time']
        ]
        print(f"{'å¹³å‡å“åº”æ—¶é—´(ç§’)':<20} {times[0]:<15.2f} {times[1]:<15.2f} {times[2]:<15.2f}")
        
        # å›å¤é•¿åº¦
        lengths = [
            self.results['baseline']['statistics']['avg_response_length'],
            self.results['rag_only']['statistics']['avg_response_length'],
            self.results['full_system']['statistics']['avg_response_length']
        ]
        print(f"{'å¹³å‡å›å¤é•¿åº¦(å­—)':<20} {lengths[0]:<15.0f} {lengths[1]:<15.0f} {lengths[2]:<15.0f}")
        
        # æˆåŠŸç‡
        success_rates = [
            self.results['baseline']['statistics']['successful_responses'] / self.results['baseline']['statistics']['total_questions'] * 100,
            self.results['rag_only']['statistics']['successful_responses'] / self.results['rag_only']['statistics']['total_questions'] * 100,
            self.results['full_system']['statistics']['successful_responses'] / self.results['full_system']['statistics']['total_questions'] * 100
        ]
        print(f"{'æˆåŠŸç‡(%)':<20} {success_rates[0]:<15.1f} {success_rates[1]:<15.1f} {success_rates[2]:<15.1f}")
    
    def save_results(self, output_dir: str = None) -> str:
        """ä¿å­˜ç»“æœ"""
        if output_dir is None:
            output_dir = self.comparison_config['output']['results_dir']
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = output_path / f"comparison_{timestamp}.json"
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'metadata': {
                'timestamp': timestamp,
                'num_questions': len(self.test_questions),
                'configurations': self.comparison_config['configurations']
            },
            'results': self.results
        }
        
        # ä¿å­˜
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {result_file}")
        
        return str(result_file)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œä¸‰ç³»ç»Ÿå¯¹æ¯”å®éªŒï¼ˆæœŸæœ«ä½œä¸šç‰ˆï¼‰')
    parser.add_argument('--num-questions', type=int, default=30,
                       help='æµ‹è¯•é—®é¢˜æ•°é‡ï¼ˆé»˜è®¤30ï¼‰')
    parser.add_argument('--config', type=str, default='configs/config.yaml',
                       help='ç³»ç»Ÿé…ç½®æ–‡ä»¶')
    parser.add_argument('--skip-manual', action='store_true',
                       help='è·³è¿‡äººå·¥è¯„ä¼°')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå®éªŒ
        experiment = SimpleComparisonExperiment(args.config)
        
        # è¿è¡Œæ‰€æœ‰é…ç½®
        experiment.run_all_configurations(args.num_questions)
        
        # æ‰“å°å¯¹æ¯”
        experiment.print_comparison()
        
        # ä¿å­˜ç»“æœ
        result_file = experiment.save_results(args.output_dir)
        
        # å®Œæˆæç¤º
        print(f"\n{'='*70}")
        print(" "*25 + "âœ… å®éªŒå®Œæˆï¼")
        print(f"{'='*70}")
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
        print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
        print(f"   1. æŸ¥çœ‹è¯¦ç»†ç»“æœ: cat {result_file}")
        print(f"   2. ç”Ÿæˆå¯è§†åŒ–: python evaluation/scripts/visualize_comparison.py {result_file}")
        
        if not args.skip_manual:
            print(f"   3. è¿›è¡Œäººå·¥è¯„ä¼°: python evaluation/scripts/manual_evaluation.py {result_file}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâœ— å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
